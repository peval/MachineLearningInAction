第11章 使用Apriori算法进行关联分析 
==========================================

从大规模数据集中寻找物品间的隐含关系被称作**关联分析（association analysis）**或者**关联规则学习（association rule learning）**。这里的主要问题在于，寻找物品的不同组合是一项十分耗时的任务，所需的计算代价很高，蛮力搜索方法并不能解决这个问题，所以需要用更智能的方法在合理的时间范围内找到频繁项集。

关联分析可以用于回答"哪些商品经常被同时购买？"之类的问题。书中举了一些关联分析的例子:

- 通过查看哪些商品经常在一起购买，可以帮助商店了解用户的购买行为。这种从数据海洋中抽取的知识可以用于商品定价、市场促销、存活管理等环节。

- 在美国国会投票记录中发现关联规则。在一个国会投票记录的数据集中发现议案投票的相关性，（原文：这里只是出于娱乐的目的，不过也可以……）使用分析结果来为政治竞选活动服务，或者预测选举官员会如何投票。

- 发现毒蘑菇的相似特征。这里只对包含某个特定元素（有毒性）的项集感兴趣，从中寻找毒蘑菇中的一些公共特征，利用这些特征来避免吃到那些有毒蘑菇。

- 在Twitter源中发现一些共现词。对于给定搜索词，发现推文中频繁出现的单词集合。

- 从新闻网站点击流中挖掘新闻流行趋势，挖掘哪些新闻广泛被用户浏览到。

- 搜索引擎推荐，在用户输入查询词时推荐同相关的查询词项。

# 1 关联分析

关联分析是在大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式：

- 频繁项集

- 关联规则

**频繁项集（frequent item sets)** 是经常出现在一块儿的物品的集合，**关联规则（association rules)** 暗示两种物品之间可能存在很强的关系。

下面用一个例子来说明这两种概念：图1给出了某个杂货店的交易清单。

交易号码|    商品
------- | ------------
0       |  豆奶，莴苣
1       |  莴苣，尿布，葡萄酒，甜菜
2       |  豆奶，尿布，葡萄酒，橙汁
3       |  莴苣，豆奶，尿布，葡萄酒
4       |  莴苣，豆奶，尿布，橙汁

    图1 某杂货店交易清单

**频繁项集是指那些经常出现在一起的商品集合**，图中的集合{葡萄酒,尿布,豆奶}就是频繁项集的一个例子。从这个数据集中也可以找到诸如尿布->葡萄酒的关联规则，即如果有人买了尿布，那么他很可能也会买葡萄酒。

我们用**支持度**和**可信度**来度量这些有趣的关系。一个**项集的支持度（support)**被定义数据集中包含该项集的记录所占的比例。如上图中，{豆奶}的支持度为4/5，{豆奶,尿布}的支持度为3/5。支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小值尺度的项集。

**可信度或置信度（confidence）**是针对**关联规则**来定义的。规则{尿布}➞{啤酒}的可信度被定义为"支持度({尿布,啤酒})/支持度({尿布})"，由于{尿布,啤酒}的支持度为3/5，尿布的支持度为4/5，所以"尿布➞啤酒"的可信度为3/4。这意味着对于包含"尿布"的所有记录，我们的规则对其中75%的记录都适用。 **规则{尿布}➞{啤酒} 可理解为条件概率，已知在购买尿布的情况下，还购买啤酒的概率**。

# 2 Apriori原理
假设我们有一家经营着4种商品（商品0，商品1，商品2和商品3）的杂货店，2图显示了所有商品之间所有的可能组合：

![杂货店所有商品之间所有的可能组合](杂货店所有商品之间所有的可能组合.png)

图2 集合{0,1,2,3,4}中所有可能的项集组合

对于包含N中物品的数据集共有$2^N - 1$种项集组合，重复上述计算过程是不现实的。

研究人员发现一种所谓的**Apriori原理，可以帮助我们减少计算量**。**Apriori原理是说如果某个项集是频繁的，那么它的所有子集也是频繁的**。更常用的是它的逆否命题，即**如果一个项集是非频繁的，那么它的所有超集也是非频繁的**。

在图3中，已知阴影项集{2,3}是非频繁的。利用这个知识，我们就知道项集{0,2,3}，{1,2,3}以及{0,1,2,3}也是非频繁的。也就是说，一旦计算出了{2,3}的支持度，知道它是非频繁的后，就可以紧接着排除{0,2,3}、{1,2,3}和{0,1,2,3}。

![Apriori原理](Apriori原理.png)

图3 图中给出了所有可能的项集，其中非频繁项集用灰色表示。

# 3 使用Apriori算法来发现频繁集

前面提到，关联分析的目标包括两项：发现频繁项集和发现关联规则。首先需要找到频繁项集，然后才能获得关联规则（正如前文所讲，计算关联规则的可信度需要用到频繁项集的支持度）。

Apriori算法是发现频繁项集的一种方法。Apriori算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个元素的项集列表。接着扫描数据集来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉。然后，对剩下来的集合进行组合以生成包含两个元素的项集。接下来，再重新扫描交易记录，去掉不满足最小支持度的项集。该过程重复进行直到所有项集都被去掉。

# 3.1 生成候选项集
数据集扫描的伪代码大致如下：

```code
对数据集中的每条交易记录tran：
    对每个候选项集can：
        检查can是否是tran的子集
        如果是，则增加can的计数
对每个候选项集：
    如果其支持度不低于最小值，则保留该项集
返回所有频繁项集列表
```

下面看一下实际代码，建立一个apriori.py文件并加入一下代码：

```python
#!/usr/bin/env python
# encoding=utf8

import numpy as np

def loadDataSet():
    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]

def createC1(dataSet):
    '''
    dataSet为全部数据集
    返回元素个数为1的项集,如每个单独商品(去重后)列表
    '''
    C1 = [] #C1即为元素个数为1的项集,如每个单独商品列表
    for transaction in dataSet:
        for item in transaction:
            if [item] not in C1:
                C1.append([item])
    C1.sort()
    C1 = map(frozenset ,C1)  # map(frozenset, C1)的语义是将C1由Python列表转换为不变集合（frozenset，Python中的数据结构）。
    #frozenset是不可变的，用户不能修改。这里使用frozenset是因为要将这些集合作为字典键值使用，使用frozenset可以实现，而set却做不到。
    return C1

def scanD(D, Ck , minSupport):
    '''
    D为全部数据集
    Ck为大小为k（包含k个元素）的候选项集
    minSupport为设定的最小支持度
    
    返回值中retList为在Ck中找出的频繁项集（支持度大于minSupport的），supportData记录各频繁项集的支持度。
    '''
    ssCnt = {} # 记录每个项集出现次数
    
    #统计每个项集出现次数
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                ssCnt[can] = ssCnt[can] + 1 if ssCnt.has_key(can) else 1
    
    numItems = float(len(D)) #全部数据集个数
    retList = []
    supportData = {}
    
    for key in ssCnt.iterkeys():
        support = ssCnt[key] / numItems
        if support >= minSupport:
            retList.insert(0,key) # 将频繁项集插入返回列表的首部。
        supportData[key] = support
    return retList, supportData



>>> dataSet = loadDataSet()
>>> dataSet
[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]
>>> C1 = createC1(dataSet)
>>> C1
[frozenset([1]), frozenset([2]), frozenset([3]), frozenset([4]), frozenset([5])]
>>> D = map(set, dataSet)
>>> D
[set([1, 3, 4]), set([2, 3, 5]), set([1, 2, 3, 5]), set([2, 5])]
>>> L1, supportData0 = scanD(D, C1, 0.5)
>>> L1
[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]
>>> supportData0
{frozenset([4]): 0.25, frozenset([5]): 0.75, frozenset([2]): 0.75, frozenset([3]): 0.75, frozenset([1]): 0.5}

```

# 3.2 完整的Apriori算法
整个Apriori算法的伪代码如下：

```code
当集合中项的个数大于0时：
    构建一个由k个项组成的候选项集的列表（k从1开始）
    计算候选项集的支持度，删除非频繁项集
    构建由k+1项组成的候选项集的列表
```

程序代码如下：

```python

```



