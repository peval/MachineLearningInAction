吴恩达机器学习整理
==================
# 1 线性回归模型 Linear Regression Model      =============> 常用于预测连续数值,找到最佳拟合直线
 项目 | 单特征线性回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1$
 参数 | $\theta_0, \theta_1$
 代价函数 | $J(\theta) =J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta_0,\theta_1) $ 

**梯度下降算法 Gradient descent algorithm:**

  repeat until convergence(收敛) {  
  
  $\theta_j = \theta_j -  \alpha \frac{\partial }{\partial \theta_j}J(\theta) $                 (for all the j)
   
 }
  
  $\theta_0 = \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)}) ;                其中 j = 0$
  
  $\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)};           其中 j \neq 0 $

-------------



 项目 | 多特征线性回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1, x_2, ... , x_n)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n  = \theta^Tx$
 参数 | $\theta_0, \theta_1, ..., \theta_n$
 代价函数 | $J(\theta) = J(\theta_0,\theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta) $ 

假设函数也可写成**向量内积的方式**:

$h_\theta (x)  == \theta^Tx$  (这里每个x样本**增加一新特征$x_0 = 1$**)

梯度下降算法**可统一**(包含j=0)为:

$\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)};$           其中 j =**0**,1,2,...,n


**模型使用时额外要点**:
 
 1. 特征归一化 -1 <= $x_i$ <= 1 或 0 <= $x_i$ <= 1 可加快收敛速度
 
 2. **最小二乘解**:通过逆矩阵求$\theta = (x^Tx)^{-1}x^Ty$, 当不可逆时，可使用伪逆函数pinv 




[机器学习实践-线性回归](https://github.com/peval/MachineLearningInAction/blob/master/Ch08_Regression/%E5%9B%9E%E5%BD%92.md)
[机器学习实践-线性回归](Ch08_Regression/%E5%9B%9E%E5%BD%92.md)


------------






# 2 逻辑回归模型 Logistic Regression Model       =============> 常用于分类classification

分类模型通常为二元分类 $y \in {0,1}$。 0为负类，1为正类(要关注的类别)

**逻辑回归要求** : $0 <= h_\theta(x) <=1$ , 线性回归$h_\theta(x) = \theta^Tx$ 没有这个限制。

因此引入sigmoid function  g(z) 将 $h_\theta(x) = g(\theta^Tx)$ 的值映射到0~1之间，得到一个概率(可能性)

常用的sigmoid function: $g(z) = $




 项目 | 逻辑回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1, x_2, ... , x_n)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n  = \theta^Tx$
 参数 | $\theta_0, \theta_1, ..., \theta_n$
 代价函数 | $J(\theta) = J(\theta_0,\theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta) $ 
