吴恩达机器学习整理
==================
# 1 线性回归模型 Linear Regression Model
 项目 | 单特征线性回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1$
 参数 | $\theta_0, \theta_1$
 代价函数 | $J(\theta) =J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta_0,\theta_1) $ 

梯度下降算法 Gradient descent algorithm
```code
repeat until convergence(收敛) {
   $\theta_j = \theta_j -  \alpha \frac{\partial }{\partial \theta_j}J(\theta)) $ (for all the j)
}

$\theta_0 = \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)}) ;其中 j = 0$
$\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x^{(i)} ;其中 j \neq 0 $
```



 项目 | 多特征线性回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1, x_2, ... , x_n)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$
 参数 | $\theta_0, \theta_1, ..., \theta_n$
 代价函数 | $J(\theta) = J(\theta_0,\theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta) $ 

梯度下降算法 Gradient descent algorithm

 
