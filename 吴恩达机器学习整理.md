吴恩达机器学习整理
==================
# 1 线性回归模型 Linear Regression Model      =============> 常用于预测连续数值,找到最佳拟合直线
 项目 | 单特征线性回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1$
 参数 | $\theta_0, \theta_1$
 代价函数 | $J(\theta) =J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta_0,\theta_1) $ 

**梯度下降算法 Gradient descent algorithm:**

  repeat until convergence(收敛) {  
  
  $\theta_j = \theta_j -  \alpha \frac{\partial }{\partial \theta_j}J(\theta) $                 (for all the j)
   
 }
  
  $\theta_0 = \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)}) ;                其中 j = 0$
  
  $\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)};           其中 j \neq 0 $

-------------



 项目 | 多特征线性回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1, x_2, ... , x_n)$
 假设 | $h_\theta (x)  = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n  = \theta^Tx$
 参数 | $\theta_0, \theta_1, ..., \theta_n$
 代价函数 | $J(\theta) = J(\theta_0,\theta_1, ..., \theta_n) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2 $
 优化目标函数Goal | $\min J(\theta) $ 

假设函数也可写成**向量内积的方式**:

$h_\theta (x)  == \theta^Tx$  (这里每个x样本**增加一新特征$x_0 = 1$**)

梯度下降算法**可统一**(包含j=0)为:

$\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)};$           其中 j =**0**,1,2,...,n


**模型使用时额外要点**:
 
 1. 特征归一化 -1 <= $x_i$ <= 1 或 0 <= $x_i$ <= 1 可加快收敛速度
 
 2. **最小二乘解**:通过逆矩阵求$\theta = (x^Tx)^{-1}x^Ty$, 当不可逆时，可使用伪逆函数pinv 




[机器学习实践-线性回归](Ch08_Regression/%E5%9B%9E%E5%BD%92.md)

------------






# 2 逻辑回归模型 Logistic Regression Model       =============> 常用于分类classification

分类模型通常为**二元分类** $y \in {0,1}$。 0为负类，1为正类(要关注的类别)

**逻辑回归要求** : $0 <= h_\theta(x) <=1$ , 线性回归$h_\theta(x) = \theta^Tx$ 没有这个限制。

因此引入**sigmoid function**  g(z) 将 $h_\theta(x) = g(\theta^Tx)$ 的值映射到0~1之间，得到一个概率(可能性)

常用的sigmoid function: $g(z) = \frac{1}{1+e^{-z}}$

![sigmoid function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)

分类依据：决策边界decision boundary (决策边界是假设函数的一个属性，取决于参数theta)
```code
suppose 
   predict "y = 1"  if h(x) >= 0.5;        <==== $\theta^Tx >= 0 $
   predict "y = 0"  if h(x) < 0.5;         <===== $\theta^Tx < 0 $
	
```


 项目 | 逻辑回归模型 |
 ---  | ----------------------
 样本 | (X,Y)  其中 $x = (x_1, x_2, ... , x_n)$, $y \in {0,1}$
 假设 | $h_\theta (x)  = \frac{1}{1+e^{-\theta^Tx}}$
 参数 | $\theta_0, \theta_1, ..., \theta_n$
 代价函数 | $cost(h_{\theta }(x),y) = -ylog(h_{\theta }(x)) + [-(1-y)log(1-h_{\theta }(x))] $
 优化目标函数Goal | $\min J(\theta) = \min \frac{1}{m}\sum_{i=1}^{m}cost(h_{\theta }(x^{(i)}),y^{(i)}) $ 


这里为什么没有使用类似于线性回归的代价函数$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(h_\theta (x^{(i)}) - y^{(i)})^2 $ ？

定义$Cost(h_\theta (x^{(i)}), y^{(i)}) = \frac{1}{2}(h_\theta (x^{(i)}) - y^{(i)})^2$ 表示真实值$y^{(i)}$与预测值$h_\theta (x^{(i)})$之间的1/2平方根误差。

因为$Cost(h_\theta (x^{(i)}), y^{(i)})$是**非凸函数non-convex function**。**凸函数才有唯一极小值** $f(\frac{x_1 + x_2}{2}) >= \frac{(f(x_1) + f(x_2))}{2}$ 
而非凸函数存在极值,**因此算法可能收敛到局部最优解，不能保证收敛到全局最小值**。

所以得寻找新的代价函数使$J(\theta)$ 满足凸函数：

![Cost](http://latex.codecogs.com/gif.latex?cost%28h_%7B%5Ctheta%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20-log%28h_%7B%5Ctheta%7D%28x%29%29%20%2C%20y%20%3D%201%5C%5C%20-log%281-h_%7B%5Ctheta%7D%28x%29%29%2C%20y%20%3D%200%20%5Cend%7Bmatrix%7D%5Cright.)

上面函数满足： $0 <= h_\theta(x) <=1$
$$"y = 1"  if h_\theta(x) >= 0.5;        <==== \theta^Tx >= 0 $$
$$"y = 0"  if h_\theta(x) < 0.5;        <==== \theta^Tx < 0 $$

如下 y=1时的Cost 函数 $-log(h_{\theta }(x))$：

![y=1时的Cost 函数](http://img.blog.csdn.net/20160406190524511)

从图可以看出，当预测出$h_\theta(x) = 1$ 时，$Cost(h_\theta (x^{(i)}), y^{(i)})$的代价为0; 而预测出$h_\theta(x) = 0$ 时，$Cost(h_\theta (x^{(i)}), y^{(i)})$的代价为$\infty $。无穷大表示：当算法预测错误时，给算法一个无穷大惩罚。



如下 y=0时的Cost 函数$-log(1-h_{\theta }(x))$：

![y=0时的Cost 函数](http://img.blog.csdn.net/20160406192947006)

从图可以看出，当预测出$h_\theta(x) = 1$ 时，$Cost(h_\theta (x^{(i)}), y^{(i)})$的代价为$\infty $; 而预测出$h_\theta(x) = 0$ 时，$Cost(h_\theta (x^{(i)}), y^{(i)})$的代价为0.

**简化CostFunction:**
$$Cost(h_{\theta }(x),y) = -ylog(h_{\theta }(x)) + [-(1-y)log(1-h_{\theta }(x))] $$  其中 y always = 0 or 1

$$ J(\theta) = \frac{1}{m}\sum_{i=1}^{m}cost(h_{\theta }(x^{(i)}),y^{(i)})$$
$$                               = -\frac{1}{m} \left [ \sum_{i=1}^{m}y^{(i)}log(h_{\theta }(x^{(i)})) +(1-y^{(i)})log(1-h_{\theta }(x^{(i)})) \right ] $$


**最小化代价函数$\min J(\theta)$, 梯度下降算法 Gradient descent algorithm:**

  repeat until convergence(收敛) {  
  
  $\theta_j = \theta_j -  \alpha \frac{\partial }{\partial \theta_j}J(\theta) $                 (for all the j)
   
 }
  
  $\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)}; $  
  
  **求偏导的过程结果与线性回归完全相同，但$h_\theta (x)$不一样。数学真奇妙，求导过程见纸质笔记第13页**
  
  =======> 向量化表达式 $ \theta = \theta - \frac{\alpha}{m}x^T(g(x\theta) - y)$

**常见的优化算法 optimization algorithm :**

    1. Gradient descent
    
    2. Conjugate gradient
    
    3. BFGS共轭梯度算法
    
    4. L-BFGS限制内存共轭梯度算法

  由于下面这3种优化算法过于复杂，NG未讲其原理。他推荐直接使用现成库中对应的优化算法函数，而不是直接使用梯度下降算法,如fminuc


# 3 过拟合over-fitting与欠拟合under-fitting的问题

    **欠拟合under-fitting** ======> 高偏差 high bias， 对于训练样本
    
    **过拟合over-fitting** ======> 高方差 high variance， $J(\theta) \approx 0 $对于训练样本能取得100%的预测效果，但无法泛化到新的数据样本中，对于新的数据预测效果不好。

参看纸质笔记page15,欠拟合与过拟合在图的上表现。

解决过拟合over-fitting的两种方法：

    1. 减少特征数量

        a. 人为手动选择要保留的特征；
        
        b. 使用维规约相关算法
    
    2. 正则化Regularization
    
        a. 保留所有特征，但减小梯度参数$\theta_j$
        
        b. 当拥有一些很有用的特征时，正则化算法表现会很好。

**正则化Regularization 所有梯度参数$\theta_j$ (不包含$\theta_0$):**

$$J(\theta) = \frac{1}{2m}\left [ \sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})^2  + \lambda \sum_{j=1}^{n} \theta_j^2 \right ] $$

引入正则化参数(regularization parameter)$\lambda$,用于决定参数$\theta_j$的代价。可用于使学习算法输出的$h_\theta(x)$更平滑，降低over-fitting的可能。但当$\theta_j$太大时，$h_\theta(x)$太平滑，萎缩到一条直线上，又导致under-fitting.


**最小化代价函数$\min J(\theta)$, 正则化的梯度下降算法 Gradient descent algorithm:**

  repeat until convergence(收敛) {  
  
  $\theta_0 = \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_0^{(i)} ;                其中 j = 0$
  
  $\theta_j = \theta_j - \alpha \frac{1}{m}\left [ \sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j \right ];           其中 j \neq 0 $
   
 }
  
整理后：

$$\theta_j = \theta_j(1 - \alpha \frac{\lambda}{m}) - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta (x^{(i)}) - y^{(i)})x_j^{(i)};  $$

其中(1 - \alpha \frac{\lambda}{m}) will always be less than 1.通常比1小一点点，eg.0.99 。用于压缩一点$\theta_j$。

**向量化后的$\theta$求解：**

$$ \theta = (x^Tx + \lambda L)^{-1}x^Ty $$

其中,![矩阵](http://latex.codecogs.com/gif.latex?L%20=\begin{bmatrix}%200%20&%20&%20&%20&%20&%20\\%20&%201%20&%20&%20&%20&%20\\%20&%20&%201%20&%20&%20&%20\\%20&%20&%20&%20.%20&%20&%20\\%20&%20&%20&%20&%20.%20&%20\\%20&%20&%20&%20&%20&%20.%20\\%20&%20&%20&%20&%20&%20&%201%20\end{bmatrix})
维(n+1) *(n+1)
