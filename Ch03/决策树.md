第三章 决策树
=================

# 1 工作原理与流程
决策树原理类似于20个问题的游戏。规则很简单：参与游戏的一方确定一个事物，其他参与者向他提问，只允许提20个问题，问题的答案只能用对与错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。

决策树的一个重要任务是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则。**根据数据集创建规则的过程**，就是机器学习的过程。专家系统中经常使用决定树，而且决策树给出的结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。

**优点**： 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

**缺点**： 可能会产生过度匹配问题。

**适用数据范围**：数值型与标称型。

_________________
# 2 决策树的构造

### 如何从一堆原始数据中构造决策树？

- 当前数据集上哪个特征在划分数据分类时起决定性作用或作用最大？必须评估每个特征，找出决定性最好的特征。
- 根据特征划分为几个数据子集（不同分支下）。如果某个分支下的数据属于同一类型，则不用再细分。若数据子集内的数据不属于同一类型，则重复第一步选择决定性最好的特征，直到所有具有相同类型的数据均在一个数据子集内。

**创建分支的伪代码createBranch()如下：**

```code

检测数据集中的每个子项是否属于同一分类：
	IF SO RETURN 类标签；
	ELSE：
		寻找划分数据集的最好特征
		划分数据集
		创建分支节点
		FOR 每个划分的子集
			调用函数createBranch()并增加返回结果到分支节点中
		RETURN  分支节点

```
一般的决策树采用二分法划分数据，但本书不使用这种方法。如果依据某属性划分数据将会产生4个可能的值，则我们将数据划分成4块，并创建4个不同的分支。本书使用ID3算法划分数据集，参考[ID3算法介绍](https://en.wikipedia.org/wiki/ID3_algorithm)

测试数据包含5个海洋生物，特征包括不浮出水面是否可以生存与是否有脚蹼。我们将这些动物分成两类：鱼类、非鱼类。

** 海洋生物数据**

id   | 不浮出水面是否可以生存 | 是否有脚蹼 | 是否属于鱼类  
 --- | ---------------------- | ---------  | ------------------- 
 1   |           是           |      是    |       是           
 2   |           是           |      是    |       是           
 3   |           是           |      否    |       否            
 4   |           否           |      是    |       否            
 5   |           否           |      是    |       否            

## 2.1 信息增益

划分数据集的**最大原则**是：将无序的数据变得更加有序。在划分数据集之前之后信息发生的变化称为**信息增益(information gain)**，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的划分选择。

集合信息的度量方式称为**香农熵或者熵(entropy)**. **熵定义为信息的期望值**。如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为

$$l\left(x_i\right)=-log_2p\left(x_i\right)$$

其中$p(x_i)$是选择该分类的概率

为了计算熵，我们要计算所有类别所有可能值包含的信息期望值，通过下面公式得到：

$$H=-\sum_{i=1}^{n}{p(x_i)log_2p\left(x_i\right)}$$

其中n是分类的数目。

下面使用Python计算信息集的信息熵：
```python
from math import log
def calcShannonEnt(dataset):
    '''
    计算dataset数据集的熵（香农，信息熵）
    '''
    numEntries = len(dataset)
    labelCounts = {}
    
    for featVec in dataset:
        label = featVec[-1]
        if label not in labelCounts.keys():
            labelCounts[label] = 0
        
        labelCounts[label] +=1
            
    shannonEnt = 0.0
    for label in labelCounts:
        prob = float(labelCounts[label]) / numEntries #使用所有类标签的发生频率计算类别出现的概率
        shannonEnt -= prob * log(prob, 2)
        
    return shannonEnt
```

代码非常简单，其中prob为某一标签出现的次数除以总数据集的大小，也就是此标签出现的概率。

创建简单的鱼鉴定数据集：

```python
def createDataset():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no'],
               ]
    labels = ['no surfacing', 'flippers']
    
    return dataSet, labels
```

计算鱼鉴定数据集的信息熵：如下增加第三个maybe分类,信息熵由0.97增加到1.37。因此熵越高，则混合的数据也越多。

```python
>>> myDat, labels = createDataset()
>>> print calcShannonEnt(myDat)
0.970950594455
>>> myDat[0][-1] = 'maybe'
>>> print myDat
[[1, 1, 'maybe'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
>>> print calcShannonEnt(myDat)
1.37095059445
```

另一个度量集合无序程序的方法是**基尼不纯度（Gini impurity）**,也就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。本书暂时不介绍。

## 2.2 划分数据集


![聊天完全](http://chart.googleapis.com/chart?cht=tx&chl=\Large%20x=\frac{-b\pm\sqrt{b^2-4ac}}{2a})

$$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$

