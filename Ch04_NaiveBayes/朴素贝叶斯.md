第4章 基于概率论的分类方法：朴素贝叶斯（NB）
==========================================
前两章要求分类器给出明确的答案，但分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。

第3章在计算特征值取某个值的概率时涉及了一些概率知识，在那里我们先统计特征在数据集中取某个特征值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。

本章会给出一些使用概率论进行分类的方法--朴素贝叶斯分类器

**优点**：在数据较少的情况下仍然有效，可以处理多类别问题。

**缺点**：对于输入数据的准备方式比较敏感。

**适用数据范围**：标称型数据。

# 1 基于贝叶斯决策理论的分类方法
 
贝叶斯决策理论的**核心思想**：选择具有最高概率的决策。

我们用p1(x, y) 表示数据点(x, y)属于类别1的概率，p2(x, y) 表示数据点(x, y)属于类别2的概率,那么对于一个新数据点(x, y),可以用下面的规则判断它的类别：

- 如果p1(x, y) > p2(x, y) , 那么类别为1。
- 如果p2(x, y) > p1(x, y) , 那么类别为2.

**注: 贝叶斯概率引入先验知识和逻辑推理来处理不确定命题。另一种概率解释称为频数概率(frequency probability)只是从数据本身获得结论，并不考虑逻辑推理及先验知识。**

# 2 条件概率

假设一个桶里装有7块石头，其中3块灰色，4块黑色。如果从桶里随机取出一块石头，那么是灰色的可能性是多少？

	由于取石头有7种可能，其中3种是灰色，所以取出灰色石头的概率是3/7

如果7块石头是放在两个桶里，A桶2灰2黑、B桶1灰2黑.条件概率(conditional probability) ,计算从B桶中取到灰色石头的概率，记为P(灰|B),称为“在已经石头出处B桶的条件下，取出灰色石头的概率”。 不难看出P(灰|B) = 1/3 , P(灰|A) = 2/4.

条件概率的计算公式如下所示：

	P(灰|B) = P(灰 and B) / P(B)

首先，用B桶中灰色石头的个数除以两个桶中总的石头数，得到P(灰 and B) = 1/7。其次，由于B桶中有3块石头，而总石头数为7，于是P(B)就等于 3/7. 于是有P(灰|B) = P(灰 and B) / P(B) = (1/7) / (3/7) = 1/3。

**贝叶斯准则**：如何交换条件概率的条件与结果，即如果已经P(x|c),要求P(c|x),那么可以使用下面的计算方法:

$$p(c|x) = \frac{p(x|c)p(c)}{p(x)}$$

# 3 使用条件概率来分类

在第1节提到了贝叶斯决策理论要求计算两个概率p1(x, y) 和p2(x, y):

- 如果p1(x, y) > p2(x, y) , 那么类别为1。
- 如果p2(x, y) > p1(x, y) , 那么类别为2.

但这两个准则并不是贝叶斯决策理论的所有内容。使用p1( )与 p2( )只是为了简化描述，而真正需要计算和比较的是$p(c_1|x, y)$和$p(c_2|x, y)$。这些符号所代表的具体意义是：给定某个由x,y表示的数据点，那么该数据点来自类别$c_1$或$c_2$的概率分别是多少？注意这些概率与之前给出的概率$p(x,y|c_1)$并不一样，不过可以使用贝叶斯准则来交换概率中的条件与结果。

$$p(c_i|x,y) = \frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$

使用这些定义，可以定义贝叶斯分类准则为：

- 如果$p(c_1|x, y) > p(c_2|x, y)$, 那么类别为1。
- 如果$p(c_2|x, y) > p(c_1|x, y)$ , 那么类别为2.

使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。

# 4 使用朴素贝叶斯进行文档分类

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（eg一封电子邮件）是实例。我们观察文档中出现的词，并把每个词的出现或不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。据估计，仅在英语中，单词的总数就有500 000之多。

假设词汇表中有1000个单词，要得到好的概率分布，就需要足够的数据样本，假定样本数为N.由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要$N^{10}$ 个样本，对于包含1000个特征的词汇表将需要$N^{1000}$个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。

如果特征之间相互独立，那么样本数就可以从$N^{1000}$减少到1000*N。所谓**独立**指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他相邻单词没有关系。但实际上是相关的，如中华人民共和国，这几个词就是相关的。这个假设正是朴素贝叶斯分类器中**朴素**(naive)一词的含义。朴素贝叶斯分类器中的另一个假设是：**每个特征同等重要，权重相同**。现实情况下这个假设也是有问题的。

# 5 使用Python进行文本分类

先从文本中获取特征，进行分词。词条可以是单词，也可以是非单词词条，如URL、IP地址或任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文本中，0表示词条未出现。

以在线留言板为例，我们要屏蔽侮辱性的言论。所有留言被分成两类：侮辱类与非侮辱类。

## 5.1 准备数据：从文本中构建词向量。

我们将文本看成单词向量或词条向量，考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。




