第4章 基于概率论的分类方法：朴素贝叶斯（NB）
==========================================
前两章要求分类器给出明确的答案，但分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。

第3章在计算特征值取某个值的概率时涉及了一些概率知识，在那里我们先统计特征在数据集中取某个特征值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。

本章会给出一些使用概率论进行分类的方法--朴素贝叶斯分类器

**优点**：在数据较少的情况下仍然有效，可以处理多类别问题。

**缺点**：对于输入数据的准备方式比较敏感。

**适用数据范围**：标称型数据。

# 1 基于贝叶斯决策理论的分类方法
 
贝叶斯决策理论的**核心思想**：选择具有最高概率的决策。

我们用p1(x, y) 表示数据点(x, y)属于类别1的概率，p2(x, y) 表示数据点(x, y)属于类别2的概率,那么对于一个新数据点(x, y),可以用下面的规则判断它的类别：

- 如果p1(x, y) > p2(x, y) , 那么类别为1。
- 如果p2(x, y) > p1(x, y) , 那么类别为2.

**注: 贝叶斯概率引入先验知识和逻辑推理来处理不确定命题。另一种概率解释称为频数概率(frequency probability)只是从数据本身获得结论，并不考虑逻辑推理及先验知识。**

# 2 条件概率

假设一个桶里装有7块石头，其中3块灰色，4块黑色。如果从桶里随机取出一块石头，那么是灰色的可能性是多少？

	由于取石头有7种可能，其中3种是灰色，所以取出灰色石头的概率是3/7

如果7块石头是放在两个桶里，A桶2灰2黑、B桶1灰2黑.条件概率(conditional probability) ,计算从B桶中取到灰色石头的概率，记为P(灰|B),称为“在已经石头出处B桶的条件下，取出灰色石头的概率”。 不难看出P(灰|B) = 1/3 , P(灰|A) = 2/4.

条件概率的计算公式如下所示：

	P(灰|B) = P(灰 and B) / P(B)

首先，用B桶中灰色石头的个数除以两个桶中总的石头数，得到P(灰 and B) = 1/7。其次，由于B桶中有3块石头，而总石头数为7，于是P(B)就等于 3/7. 于是有P(灰|B) = P(灰 and B) / P(B) = (1/7) / (3/7) = 1/3。

**贝叶斯准则**：如何交换条件概率的条件与结果，即如果已经P(x|c),要求P(c|x),那么可以使用下面的计算方法:

$$p(c|x) = \frac{p(x|c)p(c)}{p(x)}$$

# 3 使用条件概率来分类

在第1节提到了贝叶斯决策理论要求计算两个概率p1(x, y) 和p2(x, y):

- 如果p1(x, y) > p2(x, y) , 那么类别为1。
- 如果p2(x, y) > p1(x, y) , 那么类别为2.

但这两个准则并不是贝叶斯决策理论的所有内容。使用p1( )与 p2( )只是为了简化描述，而真正需要计算和比较的是$p(c_1|x, y)$和$p(c_2|x, y)$。这些符号所代表的具体意义是：给定某个由x,y表示的数据点，那么该数据点来自类别$c_1$或$c_2$的概率分别是多少？注意这些概率与之前给出的概率$p(x,y|c_1)$并不一样，不过可以使用贝叶斯准则来交换概率中的条件与结果。

$$p(c_i|x,y) = \frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$

使用这些定义，可以定义贝叶斯分类准则为：

- 如果$p(c_1|x, y) > p(c_2|x, y)$, 那么类别为1。
- 如果$p(c_2|x, y) > p(c_1|x, y)$ , 那么类别为2.

使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。

# 4 使用朴素贝叶斯进行文档分类

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（eg一封电子邮件）是实例。我们观察文档中出现的词，并把每个词的出现或不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。据估计，仅在英语中，单词的总数就有500 000之多。

假设词汇表中有1000个单词，要得到好的概率分布，就需要足够的数据样本，假定样本数为N.由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要$N^{10}$ 个样本，对于包含1000个特征的词汇表将需要$N^{1000}$个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。

如果特征之间相互独立，那么样本数就可以从$N^{1000}$减少到1000*N。所谓**独立**指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他相邻单词没有关系。但实际上是相关的，如中华人民共和国，这几个词就是相关的。这个假设正是朴素贝叶斯分类器中**朴素**(naive)一词的含义。朴素贝叶斯分类器中的另一个假设是：**每个特征同等重要，权重相同**。现实情况下这个假设也是有问题的。

# 5 使用Python进行文本分类

先从文本中获取特征，进行分词。词条可以是单词，也可以是非单词词条，如URL、IP地址或任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文本中，0表示词条未出现。

以在线留言板为例，我们要屏蔽侮辱性的言论。所有留言被分成两类：侮辱类与非侮辱类。

## 5.1 准备数据：从文本中构建词向量。

我们将文本看成单词向量或词条向量，考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。

朴素贝叶分类器通常有两种实现方式：a、基于贝努利模型实现。b.基于多项式模型实现。贝努利模型实现：并不考虑词在文档中出现的次数，只考虑出不出现。因此相当于假设词是等权重的。多项式模型实现，它考虑词在文档中的出现次数。

```python
def loadDataSet():
    '''
    训练数据集与类别标签
    '''
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0,1,0,1,0,1]  #1 代表侮辱性文字 ， 0代表正常言论
    return postingList, classVec

def createVocabList(dataSet):
    '''
    根据训练数据集生成词汇表
    '''
    vocabSet = set([])
    for document in dataSet:
        vocabSet = vocabSet | set(document) #集合合并，集合不同于list，会自动删除相同的元素。
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    '''
    将文本转化成词向量
    vocabList:词汇表
    inputSet: 某文本
    '''
    returnVec = [0]*len(vocabList) #初始化一个所有元素都是0的，有词汇表等长的词向量。0表示未出现对应词汇
    
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print "the word: %s not in vocabulary list" % word
        
    return returnVec
```

函数loadDataSet创建一些实现样本。返回的第一个变量就是进行词条切分后的文档集合，第二个变量是一个类别标签的集合，侮辱性文字或正常言论，这些类别是由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。

createVocabList函数会创建一个包含在所有文档中出现的不重复词的列表。

setOfWords2Vec函数在词汇表的基础上，将输入的某个文档转化成文档向量，向量的每个元素为1或0，分别表示词汇表中的单词在输入文档中是否出现。

```python
>>> postingList, classVec = loadDataSet()
>>> myVocabList = createVocabList(postingList)
>>> print myVocabList # myVocabList词汇表中已经不会出现重复的单词。目前该词表还没有排序，需要的话，稍后可以对其排序。
['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'] 
>>> print setOfWords2Vec(myVocabList, postingList[0])
[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]
>>> print setOfWords2Vec(myVocabList, postingList[1])
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0]
```

# 5.2 训练算法：从词向量计算概率

前面介绍了如何将一组单词转换成一组数字，接下来看看如何使用这些数字计算概率。我们将前面
