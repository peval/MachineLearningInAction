第4章 基于概率论的分类方法：朴素贝叶斯（NB）
==========================================
前两章要求分类器给出明确的答案，但分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时给出这个猜测的概率估计值。

第3章在计算特征值取某个值的概率时涉及了一些概率知识，在那里我们先统计特征在数据集中取某个特征值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。

本章会给出一些使用概率论进行分类的方法--朴素贝叶斯分类器

**优点**：在数据较少的情况下仍然有效，可以处理多类别问题。

**缺点**：对于输入数据的准备方式比较敏感。

**适用数据范围**：标称型数据。

# 1 基于贝叶斯决策理论的分类方法
 
贝叶斯决策理论的**核心思想**：选择具有最高概率的决策。

我们用p1(x, y) 表示数据点(x, y)属于类别1的概率，p2(x, y) 表示数据点(x, y)属于类别2的概率,那么对于一个新数据点(x, y),可以用下面的规则判断它的类别：

- 如果p1(x, y) > p2(x, y) , 那么类别为1。
- 如果p2(x, y) > p1(x, y) , 那么类别为2.

**注: 贝叶斯概率引入先验知识和逻辑推理来处理不确定命题。另一种概率解释称为频数概率(frequency probability)只是从数据本身获得结论，并不考虑逻辑推理及先验知识。**

# 2 条件概率

假设一个桶里装有7块石头，其中3块灰色，4块黑色。如果从桶里随机取出一块石头，那么是灰色的可能性是多少？

	由于取石头有7种可能，其中3种是灰色，所以取出灰色石头的概率是3/7

如果7块石头是放在两个桶里，A桶2灰2黑、B桶1灰2黑.条件概率(conditional probability) ,计算从B桶中取到灰色石头的概率，记为P(灰|B),称为“在已经石头出处B桶的条件下，取出灰色石头的概率”。 不难看出P(灰|B) = 1/3 , P(灰|A) = 2/4.

条件概率的计算公式如下所示：

	P(灰|B) = P(灰 and B) / P(B)

首先，用B桶中灰色石头的个数除以两个桶中总的石头数，得到P(灰 and B) = 1/7。其次，由于B桶中有3块石头，而总石头数为7，于是P(B)就等于 3/7. 于是有P(灰|B) = P(灰 and B) / P(B) = (1/7) / (3/7) = 1/3。

**贝叶斯准则**：如何交换条件概率的条件与结果，即如果已经P(x|c),要求P(c|x),那么可以使用下面的计算方法:

$$p(c|x) = \frac{p(x|c)p(c)}{p(x)}$$

# 3 使用条件概率来分类

在第1节提到了贝叶斯决策理论要求计算两个概率p1(x, y) 和p2(x, y):

- 如果p1(x, y) > p2(x, y) , 那么类别为1。
- 如果p2(x, y) > p1(x, y) , 那么类别为2.

但这两个准则并不是贝叶斯决策理论的所有内容。使用p1( )与 p2( )只是为了简化描述，而真正需要计算和比较的是$p(c_1|x, y)$和$p(c_2|x, y)$。这些符号所代表的具体意义是：给定某个由x,y表示的数据点，那么该数据点来自类别$c_1$或$c_2$的概率分别是多少？注意这些概率与之前给出的概率$p(x,y|c_1)$并不一样，不过可以使用贝叶斯准则来交换概率中的条件与结果。

$$p(c_i|x,y) = \frac{p(x,y|c_i)p(c_i)}{p(x,y)}$$

使用这些定义，可以定义贝叶斯分类准则为：

- 如果$p(c_1|x, y) > p(c_2|x, y)$, 那么类别为1。
- 如果$p(c_2|x, y) > p(c_1|x, y)$ , 那么类别为2.

使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。

# 4 使用朴素贝叶斯进行文档分类

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（eg一封电子邮件）是实例。我们观察文档中出现的词，并把每个词的出现或不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。据估计，仅在英语中，单词的总数就有500 000之多。

假设词汇表中有1000个单词，要得到好的概率分布，就需要足够的数据样本，假定样本数为N.由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要$N^{10}$ 个样本，对于包含1000个特征的词汇表将需要$N^{1000}$个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。

如果特征之间相互独立，那么样本数就可以从$N^{1000}$减少到1000*N。所谓**独立**指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他相邻单词没有关系。但实际上是相关的，如中华人民共和国，这几个词就是相关的。这个假设正是朴素贝叶斯分类器中**朴素**(naive)一词的含义。朴素贝叶斯分类器中的另一个假设是：**每个特征同等重要，权重相同**。现实情况下这个假设也是有问题的。

# 5 使用Python进行文本分类

先从文本中获取特征，进行分词。词条可以是单词，也可以是非单词词条，如URL、IP地址或任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文本中，0表示词条未出现。

以在线留言板为例，我们要屏蔽侮辱性的言论。所有留言被分成两类：侮辱类与非侮辱类。

## 5.1 准备数据：从文本中构建词向量。

我们将文本看成单词向量或词条向量，考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。

朴素贝叶分类器通常有两种实现方式：a、基于贝努利模型实现。b.基于多项式模型实现。贝努利模型实现：并不考虑词在文档中出现的次数，只考虑出不出现。因此相当于假设词是等权重的。多项式模型实现，它考虑词在文档中的出现次数。

```python
def loadDataSet():
    '''
    训练数据集与类别标签
    '''
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0,1,0,1,0,1]  #1 代表侮辱性文字 ， 0代表正常言论
    return postingList, classVec

def createVocabList(dataSet):
    '''
    根据训练数据集生成词汇表
    '''
    vocabSet = set([])
    for document in dataSet:
        vocabSet = vocabSet | set(document) #集合合并，集合不同于list，会自动删除相同的元素。
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    '''
    将文本转化成词向量
    vocabList:词汇表
    inputSet: 某文本
    '''
    returnVec = [0]*len(vocabList) #初始化一个所有元素都是0的，有词汇表等长的词向量。0表示未出现对应词汇
    
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else:
            print "the word: %s not in vocabulary list" % word
        
    return returnVec
```

函数loadDataSet创建一些实现样本。返回的第一个变量就是进行词条切分后的文档集合，第二个变量是一个类别标签的集合，侮辱性文字或正常言论，这些类别是由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。

createVocabList函数会创建一个包含在所有文档中出现的不重复词的列表。

setOfWords2Vec函数在词汇表的基础上，将输入的某个文档转化成文档向量，向量的每个元素为1或0，分别表示词汇表中的单词在输入文档中是否出现。

```python
>>> postingList, classVec = loadDataSet()
>>> myVocabList = createVocabList(postingList)
>>> print myVocabList # myVocabList词汇表中已经不会出现重复的单词。目前该词表还没有排序，需要的话，稍后可以对其排序。
['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'] 
>>> print setOfWords2Vec(myVocabList, postingList[0])
[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]
>>> print setOfWords2Vec(myVocabList, postingList[1])
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0]
```

# 5.2 训练算法：从词向量计算概率

前面介绍了如何将一组单词转换成一组数字，接下来看看如何使用这些数字计算概率。我们将前面朴素贝叶斯准则中的(x,y)替换与$w$,$w$表示这是一个向量，它由多个数值组成。

$$p(c_i|w) = \frac{p(w|c_i)p(c_i)}{p(w)}$$

概率$p(c_i)$可通过类别i(侮辱性留言或非侮辱性留言)中文档数除以总的文档数来计算。接下来计算$p(w|c_i)$,这里就要用到朴素贝叶斯假设。**如果将$w$展开为一个个独立特征，那么上述概率就可以写成$p(w_0,w_1,w_2,...,w_N|c_i)$,这里假设所有词都互相独立，这个假设也称作条件独立性假设，它意味着可以使用$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_N|c_i)$来计算上述概率。这就极大的简化了计算过程。

该函数的伪代码如下:

```code
计算每个类别中的文档数目
对每篇训练文档：
	对每个类别：
		如果词条出现在文档中-->增加该词条的计数值
		增加所有词条的计数值
	对每个类别：
		对每个词条：
			将该词条的数目除以总词条数据得到条件概率
	返回每个类别的条件概率
```

这里会使用到Numpy中的一些函数，提前导入numpy.朴素贝叶斯分类器训练函数：

```python
def trainNB0(trainMatrix, trainCategory):
    '''
    朴素贝叶斯分类器训练函数
    trainMatrix:文档矩阵
    trainCategory: 由每篇文档类别标签所构成的向量。
    '''
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = np.sum(trainCategory) / float(numTrainDocs) #计算侮辱性文档的概率，因为trainCategory取值0或1，1表示侮辱性文档。sum即为侮辱性文档个数
    #而正常文档的概率为1-pAbusive ,对于多分类就不能直接用1减了。
    
    pAbusiveNum = np.zeros(numWords) #侮辱性文档
    pNormalNum = np.zeros(numWords) #非侮辱性文档
    
    pAbusiveDenom = 0.0
    pNormalDenom = 0.0
    
    for i in range(numTrainDocs):
        if trainCategory[i] == 1: #侮辱性文档
            pAbusiveNum += trainMatrix[i] #统计所有侮辱性文档中，所有出现词的次数
            pAbusiveDenom += np.sum(trainMatrix[i])
        else:
            pNormalNum += trainMatrix[i] #统计所有正常文档中，所有出现词的次数
            pNormalDenom += np.sum(trainMatrix[i])            
    
    pAbusiveVect = pAbusiveNum / pAbusiveDenom   # change to log() 计算的都是条件概率p(w|c)
    pNormalVect = pNormalNum / pNormalDenom     # change to log()
    
    return pAbusiveVect, pNormalVect, pAbusive
```
代码输入参数trainMatrix(文档矩阵)与trainCategory(由每篇文档类别标签所构成的向量)。先计算侮辱性文档的概率$p(c_i)$,由于trainCategory取值0或1，而1表示侮辱性文档,则sum即为侮辱性文档个数。因为是二分类问题，正常文档的概率就是1-pAbusive。

计算$p(w_i|c_1)$与$p(w_i|c_0)$，通过for循环遍历训练集trainMatrix中的所有文档，分别计算每个类别中每个词汇出现的次数pAbusiveNum和总词汇出现总数pAbusiveDenom 。 最后，对每个元素除以该类别中的总词数。

```python
>>> trainMat = []
>>> for postingDoc in postingList:
        trainMat.append(setOfWords2Vec(myVocabList, postingList[0]))    
>>> pAbusiveVect, pNormalVect, pAbusive = trainNB0(trainMat, classVec)
>>> print pAbusiveVect
[ 0.          0.          0.          0.05263158  0.05263158  0.          0.
  0.          0.05263158  0.05263158  0.          0.          0.
  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.
  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316
  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]
>>> print pNormalVect
[ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667
  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667
  0.04166667  0.          0.          0.08333333  0.          0.
  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667
  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667
  0.04166667  0.125     ]
>>> print pAbusive
0.5
```

首先，侮辱性文档概率$p(c_1)$ (pAbusive)为0.5是正确的。接下来，看一看在给定文档类别条件下词汇表中单词的出现概率，是否正确？词汇表中的第一个词是cute，其在类别0中出现了1次，而在类别1中从未出现。对应的条件概率分别是0.04166667与0.0，该计算也正确。我们找到概率最大是pAbusiveVect数组第26个下标位置，大小为0.15789474。在myVocabList词汇表中对应第26下标是单词stupid。这意味着stupid最能表征类别1（侮辱性文档）的单词。

但上面这个函数还存在一些缺陷。

# 5.3 测试算法：根据现实情况修改分类器

利用朴素贝叶分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算$p(w_0|1)p(w_1|1)p(w_2|1)$。**但如果其中一个概率值为0，则最后乘积也为0.为降低这种影响，可以将所有词的出现次数初始化为1，并将分母初始化为2**.

```python
    pAbusiveNum = np.ones(numWords) #侮辱性文档
    pNormalNum = np.ones(numWords) #非侮辱性文档
    
    pAbusiveDenom = 2.0
    pNormalDenom = 2.0
```

另一个是下溢出的问题，**由于太多很小的数相乘造成的。当计算$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_N|c_i)$时，由于大部分因子都非常小，所以程序会下溢出或者得不到正确答案**。一个解决办法是对乘积取自然对数。在代数中有$ln(a*b) = ln(a) + ln(b)$,于是通过求对数可避免下溢出或者浮点数四舍五入到0的错误。同时，由于函数f(x)与ln(f(x))的曲线，在相同区域内同时增加或减少，并且在相同点上取到极值。它们取值虽然不同，但不影响最终结果。因此修改代码

```python
    pAbusiveVect = np.log(pAbusiveNum / pAbusiveDenom)  # change to log() 计算的都是条件概率p(w|c)
    pNormalVect = np.log(pNormalNum / pNormalDenom)     # change to log()
```
![f(x) 与 ln(f(x))](f\(x\)与ln\(f\(x\)\).jpg)

现在已经准备好构建完整的分类器了，接着，开始编写朴素贝叶分类函数：
```python
def classifyNB(vec2Classify, pNormalVect, pAbusiveVect, pAbusive):
    '''
    朴素贝叶分类函数
    vec2Classify:要分类的向量
    pNormalVect： 正常文档条件概率p(w|c_0)
    pAbusiveVect：侮辱性文档条件概率p(w|c_1)
    pAbusive： 侮辱性文档的概率
    '''
    p0 = np.sum(vec2Classify * pNormalVect ) + np.log(1 - pAbusive) #这里只计算了分子，由于分母相同，对于比大小无影响
    p1 = np.sum(vec2Classify * pAbusiveVect ) + np.log(pAbusive)
    
    if p1 > p0:
        return 1
    else:
        return 0
    
def testingNB():
    '''
    测试贝叶分类器的总函数
    '''
    postingList, classVec = loadDataSet()
    myVocabList = createVocabList(postingList)
    
    trainMat = []
    for postingDoc in postingList:
        trainMat.append(setOfWords2Vec(myVocabList, postingDoc))
        
    pAbusiveVect, pNormalVect, pAbusive = trainNB0(trainMat, classVec)
    
    testEntry = ['love', 'my', 'dalmation']
    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry, 'classified as: ' , classifyNB(thisDoc, pNormalVect, 
                                                   pAbusiveVect, 
                                                   pAbusive)
    # ['love', 'my', 'dalmation'] classified as:  0

    testEntry = ['stupid', 'garbage']
    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry, 'classified as: ' , classifyNB(thisDoc, pNormalVect, 
                                                    pAbusiveVect, 
                                                    pAbusive)    
    # ['stupid', 'garbage'] classified as:  1
```

在函数classifyNB中，计算p0、p1时,因为$$p0 = p(c_0|w) = \frac{p(w|c_0)p(c_0)}{p(w)}$$  =======> $$log(p0) = log(p(c_0|w)) = \log(\frac{p(w|c_0)p(c_0)}{p(w)}) = \log(p(w|c_0)) + \log(p(c_0)) - \log(p(w))$$, 同理$$log(p1) = log(p(c_1|w)) = \log(\frac{p(w|c_1)p(c_1)}{p(w)}) = \log(p(w|c_1)) + \log(p(c_1)) - \log(p(w))$$
其中根据朴素贝叶斯假设，$w_i$是相互独立事件有，
$$\log(p(w|c_1)) = \log(p(w_0|c_1)p(w_1|c_1)p(w_2|c_1)...p(w_N|c_1)) = \log(p(w_0|c_1)) + \log(p(w_1|c_1)) + \log(p(w_2|c_1)) + ... + \log(p(w_N|c_1))$$
同理
$$\log(p(w|c_0)) = \log(p(w_0|c_0)) + \log(p(w_1|c_0)) + \log(p(w_2|c_0)) + ... + \log(p(w_N|c_0))$$

由于比较p0与p1大小等同于比较$log(p0)$与$log(p1)$，而因为都减去$\log(p(w))$,因此可不计算$\log(p(w))$。最后只用比较$$\log(p(w|c_1)) + \log(p(c_1)) 与 \log(p(w|c_0)) + \log(p(c_0))$$的大小，来决定属于哪个分类。

函数testingNB封装所有朴素贝叶斯分类器操作

接下来断续对代码进行修改，使分类器工作更完善。

## 5.4 准备数据：文档词袋模型

目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为**词集模型(set-of-words model)**。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为**词袋模型（bag-of-words model）**。在词袋模型中，每个单词出现多次，而词集模型中，每个词只能出现一次。因此增加bagOfWords2Vec函数
```python
def bagOfWords2Vec(vocabList, inputSet):
    '''
    将文本转化成词向量，词袋模型，每个单词出现多
    vocabList:词汇表
    inputSet: 某文本
    '''
    returnVec = [0]*len(vocabList) #初始化一个所有元素都是0的，有词汇表等长的词向量。0表示未出现对应词汇
    
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
        else:
            print "the word: %s not in vocabulary list" % word
        
    return returnVec
```

bagOfWords2Vec函数与setOfWords2Vec函数唯一的区别是，每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设置为1.

# 6 示例：使用朴素贝叶斯过滤垃圾邮件


