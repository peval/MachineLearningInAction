第8章 预测数值型数据：回归
=========================
回归与分类的**不同**：在于其目标变量是连续数值型，而分类的目标变量是标称型数据。

# 1 用线性回归找到最佳拟合直线

**线性回归**

**优点**：结果易于理解，计算上不复杂。

**缺点**：对非线性的数据拟合不好。

**适用数据范围**：数值型和标称型数据。

**回归目的**: 预测数值型的目标值，最直接的办法是依据输入样本数据输出一个目标值的计算公式。

假如你要预测姐姐男友汽车的功率大小，可能会这么计算

    HorsePower = 0.0015*annualSalary(年薪) - 0.99*hoursListeningToPublicRadio(听广播时间)

这就是所谓的**回归方程(regression equation)**,其中0.0015与-0.99称作**回归系数(regression weights)**,求这些回归系数的过程就是**回归**。一旦有了回归系数，再给定输入时，直接用回归系统乘以输入值，再将结果全部加起来，就得到了**预测值**。

回归分为线性回归与非线性回归。一般我们使用线性回归，即可以将输入项分别乘以一些常量，再将结果加起来得到输出。非线性回归模型则认为输出可能是输入的乘积.

    HorsePower = 0.0015*annualSalary(年薪)/hoursListeningToPublicRadio

回归的一般方法

    1. 收集数据： 采用任意方法收集数据
    
    2. 准备数据：回归需要数值型数据，标称型数据将被转成二值型数据。

    3. 分析数据：给出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比。

    4. 训练算法：找到回归系数。

    5. 测试算法：使用$R^2$或者预测值和数据的拟合度，来分析模型的效果。

    6. 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。


怎么求回归方程？假定输入数据存放在矩阵$X$中，而回归系数存放在向量$w$中。那么对于给定的数据$X_1$,预测结果将会通过$Y_1 = X^{T}_{1}w$给出。现在问题是通过$X$与对应$Y$,怎样找到$w$呢?一个常用的方法就是找出使误差最小的$w$.这里误差是指预测y值和真实y值之间的差值，因为误差有正负，若直接相加可能会相互抵消，所以我们使用平方误差。

平方误差可以写做：
$$\sum_{i=1}^{m} (y_i - x_{i}^{T}w)^2$$
用矩阵表示还可以写做$(y-Xw)^{T}(y-Xw)$。如果对$w$求导，得到
$$\frac{\partial {\sum_{i=1}^{m} (y_i - x_{i}^{T}w)^2}}{\partial w} = -2\sum_{i=1}^{m} (y_i - x_{i}^{T}w)x_{i}^{T} = -2X^T(Y - Xw)$$
令其等于0，求极值，解出w如下：
$$ \hat{w} = (X^TX)^{-1}X^Ty $$
这里$\hat{w}$表示最优解，最佳估计。值得注意的是，上述公式中包含$(X^TX)^{-1}$,也就是对矩阵求逆,这个方程只在逆矩阵存在时适用。因此需要在代码中先进行判断是否存在逆矩阵。这种解法就叫做**最小二乘解**。

补充说明，求线性方程组的解，矩阵方程Xw=y
    $$x_{11}w_1 + x_{12}w_2 + ... + x_{1n}w_n = y_1$$
    $$x_{21}w_1 + x_{22}w_2 + ... + x_{2n}w_n = y_2$$
                    $$ . $$
                    $$ . $$
                    $$ . $$
    $$x_{m1}w_1 + x_{m2}w_2 + ... + x_{mn}w_n = y_m$$

如果$X$有逆矩阵（X为n*n矩阵，且秩为n,n个行向量两两线性不相关），则方程组的解为$w = X^{-1}y$。如果X没有逆矩阵，则该方程组或者没有解，或者有无穷多个解。

当逆矩阵不存在时，我们仍然想找出该方程组的最好解，意味着找出自变量的最好线性组合来预测因变量。使用线性代数的术语，我们想找到尽可能接近向量y的向量Xw;换句话说，我们希望最小化向量y - Xw的长度|| y - Xw||最小。这称作**最小二乘(least square)问题**,可以证明，方程$Xw=y$的最小二乘解是$w = (X^TX)^{-1}X^Ty$







