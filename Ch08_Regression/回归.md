第8章 预测数值型数据：回归
=========================
回归与分类的**不同**：在于其目标变量是连续数值型，而分类的目标变量是标称型数据。

# 1 用线性回归找到最佳拟合直线

**线性回归**

**优点**：结果易于理解，计算上不复杂。

**缺点**：对非线性的数据拟合不好。

**适用数据范围**：数值型和标称型数据。

**回归目的**: 预测数值型的目标值，最直接的办法是依据输入样本数据输出一个目标值的计算公式。

假如你要预测姐姐男友汽车的功率大小，可能会这么计算

    HorsePower = 0.0015*annualSalary(年薪) - 0.99*hoursListeningToPublicRadio(听广播时间)

这就是所谓的**回归方程(regression equation)**,其中0.0015与-0.99称作**回归系数(regression weights)**,求这些回归系数的过程就是**回归**。一旦有了回归系数，再给定输入时，直接用回归系统乘以输入值，再将结果全部加起来，就得到了**预测值**。

回归分为线性回归与非线性回归。一般我们使用线性回归，即可以将输入项分别乘以一些常量，再将结果加起来得到输出。非线性回归模型则认为输出可能是输入的乘积.

    HorsePower = 0.0015*annualSalary(年薪)/hoursListeningToPublicRadio

回归的一般方法

    1. 收集数据： 采用任意方法收集数据
    
    2. 准备数据：回归需要数值型数据，标称型数据将被转成二值型数据。

    3. 分析数据：给出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比。

    4. 训练算法：找到回归系数。

    5. 测试算法：使用$R^2$或者预测值和数据的拟合度，来分析模型的效果。

    6. 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。


怎么求回归方程？假定输入数据存放在矩阵$X$中，而回归系数存放在向量$w$中。那么对于给定的数据$X_1$,预测结果将会通过$Y_1 = X^{T}_{1}w$给出。现在问题是通过$X$与对应$Y$,怎样找到$w$呢?一个常用的方法就是找出使误差最小的$w$.这里误差是指预测y值和真实y值之间的差值，因为误差有正负，若直接相加可能会相互抵消，所以我们使用平方误差。

平方误差可以写做：
$$\sum_{i=1}^{m} (y_i - x_{i}^{T}w)^2$$
用矩阵表示还可以写做$(y-Xw)^{T}(y-Xw)$。如果对$w$求导，得到
$$\frac{\partial {\sum_{i=1}^{m} (y_i - x_{i}^{T}w)^2}}{\partial w} = -2\sum_{i=1}^{m} (y_i - x_{i}^{T}w)x_{i}^{T} = -2X^T(Y - Xw)$$
令其等于0，求极值，解出w如下：
$$ \hat{w} = (X^TX)^{-1}X^Ty $$
这里$\hat{w}$表示最优解，最佳估计。值得注意的是，上述公式中包含$(X^TX)^{-1}$,也就是对矩阵求逆,这个方程只在逆矩阵存在时适用。因此需要在代码中先进行判断是否存在逆矩阵。这种解法就叫做**最小二乘解**。

补充说明，求线性方程组的解，矩阵方程Xw=y
    $$x_{11}w_1 + x_{12}w_2 + ... + x_{1n}w_n = y_1$$
    $$x_{21}w_1 + x_{22}w_2 + ... + x_{2n}w_n = y_2$$
                    $$ . $$
                    $$ . $$
                    $$ . $$
    $$x_{m1}w_1 + x_{m2}w_2 + ... + x_{mn}w_n = y_m$$

如果$X$有逆矩阵（X为n*n矩阵，且秩为n,n个行向量两两线性不相关），则方程组的解为$w = X^{-1}y$。如果X没有逆矩阵，则该方程组或者没有解，或者有无穷多个解。

当逆矩阵不存在时，我们仍然想找出该方程组的最好解，意味着找出自变量的最好线性组合来预测因变量。使用线性代数的术语，我们想找到尽可能接近向量y的向量Xw;换句话说，我们希望最小化向量y - Xw的长度|| y - Xw||最小。这称作**最小二乘(least square)问题**,可以证明，方程$Xw=y$的最小二乘解是$w = (X^TX)^{-1}X^Ty$

下面来介绍如何给出该数据的最佳拟合直线。

在程序文件regression.py中增加如下代码
```python
import numpy as np

def loadDataSet(filename):
    dataMat = []; labelMat = []
    with open(filename, 'r') as fp: 
        for line in fp.readlines():
            lineAttr = line.strip().split()
            
            dataMat.append([float(attr) for attr in lineAttr[:-1]]) # 由于python数组index是以0开始，样本文件第一列都为1.0，即x0等于1.0，第二列的值为x1
            labelMat.append(float(lineAttr[-1]))
    return dataMat, labelMat

def standRegres(xArr, yArr):
    '''
    使用普通最小二乘法，来计算最佳拟合直线
    '''
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    xTx = xMat.T * xMat
    
    if np.linalg.det(xTx) == 0.0: #计算行列式，判断是否有逆矩阵。若没检查行列式是否为0就直接计算矩阵的逆，将会出现错误。另外Numpy的线性代数库还提供一个函数来解未知矩阵，修改ws = xTx.I * (xMat.T * yMat)为ws = np.linalg.solve(xTx, xMat.T*yMat)
        print "This matrix is singular , cannot do inverse"
        return
    
    ws = xTx.I * (xMat.T * yMat) # ws存放回归系数
    return ws

>>> xArr , yArr = loadDataSet('ex0.txt')
>>> xArr[0:2]
[[1.0, 0.067732], [1.0, 0.42781]]  #第一列都为1.0，即x0等于1.0，第二列的值为x1
>>> yArr[0:2]
[3.176513, 3.816464]

>>> ws = standRegres(xArr, yArr)
>>> ws # ws存放回归系数,在用内积预测y的时候，第一维将乘以前面的常数X0,第二维乘以输入变量X1。因为x0=1.0,所以y=ws[0] + ws[1] * x1.
matrix([[ 3.00774324],
        [ 1.69532264]])

```

现在绘出数据集散点图与最佳拟合直线图。y=ws[0] + ws[1] * x1.为预测值。为了和真实的y值区分开，将预测值计为yHat。
```python
def plotRegress():
    import matplotlib.pyplot as plt
    fig = plt.figure()
    ax = fig.add_subplot(111)
    
    xArr , yArr = loadDataSet('ex0.txt')
    xMat = np.mat(xArr); yMat = np.mat(yArr)
    
    ws = standRegres(xArr, yArr)
    #回归函数，用于预测
    yHat = xMat*ws
    
    #绘出数据集散点图
    ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0])
     
    xCopy = xMat.copy()
    xCopy.sort(0)
    yHat = xCopy*ws
    ax.plot(xCopy[:,1], yHat)
    plt.show()

```
最佳拟合直线
![最佳拟合直线.png](最佳拟合直线.png)

 几乎任一数据集都可以用上述方法建立模型，那么，如何判断这些模型的好坏呢？如下两个数据集散点图，具有相同的回归系统（0和2.0），上图的相关系数是0.58，下图的相关系数是0.99.
![相同回归系数的数据集但相关系数不同](相同回归系数的数据集但相关系数不同.jpg)

有种方法可以计算预测值yHat与真实值y序列的匹配程度，那就是计算这两个序列的相关系数。Numpy库中提供了相关系数的计算方法：通过函数corrcoef(yEstimate, yActual)来计算预测值yHat与真实值y序列的相关性。
```python
>>> yHat = xMat*ws #回归函数，用于预测
>>> np.corrcoef(yHat.T, yMat) #由于yHat是列向量，进行转置，以保证两个向量都是行向量。
array([[ 1.        ,  0.98647356],
       [ 0.98647356,  1.        ]])
```

可以看到输出矩阵包含所有两两组合的相关系数。对角线上的数据是1.0，因为yMat和自己的匹配就最完美的，而yHat与yMat的相关系数为0.98.

最佳拟合直线方法将数据视为直线进行建模，具在十分不错的表现。但最佳拟合直线.png的数据当中似乎还存在其他的潜在模式。那么如何才能利用这些模式呢？我们可以根据数据来局部调整预测，下面就介绍这种方法。

# 2 局部加权线性回归






